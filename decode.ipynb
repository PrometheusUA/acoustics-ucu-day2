{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from g2p_en import G2p\n",
    "\n",
    "from praatio import textgrid as tgio\n",
    "from praatio.data_classes.interval_tier import Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(wav):\n",
    "    return torchaudio.compliance.kaldi.mfcc(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, url=\"dev-clean\"):\n",
    "        super().__init__()\n",
    "        self.librispeech = torchaudio.datasets.LIBRISPEECH(\".\", url=url, download=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.librispeech)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav, sr, text, speaker_id, chapter_id, utterance_id = self.librispeech[index]\n",
    "        return make_frames(wav), text, (speaker_id, chapter_id, utterance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=13, subsample_dim=128, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.subsample = nn.Conv1d(input_dim, subsample_dim, 5, stride=4, padding=3)\n",
    "        self.lstm = nn.LSTM(\n",
    "            subsample_dim, hidden_dim, batch_first=True, num_layers=3, dropout=0.2\n",
    "        )\n",
    "\n",
    "    def subsampled_lengths(self, input_lengths):\n",
    "        # https://github.com/vdumoulin/conv_arithmetic\n",
    "        p, k, s = (\n",
    "            self.subsample.padding[0],\n",
    "            self.subsample.kernel_size[0],\n",
    "            self.subsample.stride[0],\n",
    "        )\n",
    "        o = input_lengths + 2 * p - k\n",
    "        o = torch.floor(o / s + 1)\n",
    "        return o.int()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.subsample(x.mT).mT\n",
    "        x = x.relu()\n",
    "        x, _ = self.lstm(x)\n",
    "        return x.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.g2p = G2p()\n",
    "\n",
    "        # http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n",
    "        self.rdictionary = [\"ε\", # CTC blank\n",
    "                            \" \",\n",
    "                            \"AA0\", \"AA1\", \"AE0\", \"AE1\", \"AH0\", \"AH1\", \"AO0\", \"AO1\", \"AW0\", \"AW1\", \"AY0\", \"AY1\",\n",
    "                            \"B\", \"CH\", \"D\", \"DH\",\n",
    "                            \"EH0\", \"EH1\", \"ER0\", \"ER1\", \"EY0\", \"EY1\",\n",
    "                            \"F\", \"G\", \"HH\",\n",
    "                            \"IH0\", \"IH1\", \"IY0\", \"IY1\",\n",
    "                            \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\",\n",
    "                            \"OW0\", \"OW1\", \"OY0\", \"OY1\",\n",
    "                            \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\",\n",
    "                            \"UH0\", \"UH1\", \"UW0\", \"UW1\",\n",
    "                            \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
    "\n",
    "        self.dictionary = {c: i for i, c in enumerate(self.rdictionary)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rdictionary)\n",
    "\n",
    "    def encode(self, text):\n",
    "        labels = [c.replace('2', '0') for c in self.g2p(text) if c != \"'\"]\n",
    "        targets = torch.LongTensor([self.dictionary[phoneme] for phoneme in labels])\n",
    "        return targets\n",
    "\n",
    "    def decode(self, targets):\n",
    "        return [self.rdictionary[token] for token in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recognizer(nn.Module):\n",
    "    def __init__(self, feat_dim=1024, vocab_size=55+1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(feat_dim, vocab_size)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.classifier(features)\n",
    "        return features.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "encoder = Encoder()\n",
    "recognizer = Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load('lstm_p3_360+500.pt', map_location='cpu')\n",
    "encoder.load_state_dict(ckpt['encoder'])\n",
    "recognizer.load_state_dict(ckpt['recognizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_frames, text, ids = LibriSpeech()[0]\n",
    "phonemes = vocab.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1272, 128104, 0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder(audio_frames)\n",
    "outputs = recognizer.forward(features) # (T, 55+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real phonemes:        MIH1STER0 KWIH1LTER0 IH1Z DHAH0 AH0PAA1SAH0L AH1V DHAH0 MIH1DAH0L KLAE1SAH0Z AH0ND WIY1 AA1R GLAE1D TUW1 WEH1LKAH0M HHIH1Z GAA1SPAH0L\n",
      "predicted phonemes:  εεεεεεεεεεεεεεεεεεεεMIH1STER0  KRIH1LTTεER0ε  IH1Z DHAH0  εAH0εPPεAA1SεAH0AH0LL  AH1V DHAH0 MIH1DAH0LL KLLAE1εSεAH0εεZεε  εAH0ND WIH1εRR GLLAE1εDDε TεUW1WWEH1LεKεAH0M  HHHHIH1Z   GεAA1εSPPεAH0Lεεεεεεεε\n"
     ]
    }
   ],
   "source": [
    "print('real phonemes:       ', ''.join(vocab.decode(phonemes)))\n",
    "print(f'predicted phonemes: ', ''.join(vocab.decode(torch.argmax(outputs, dim=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_frames, text, ids in LibriSpeech():\n",
    "    features = encoder(audio_frames)\n",
    "    outputs = recognizer.forward(features)\n",
    "\n",
    "    tg = tgio.Textgrid()\n",
    "    tg.minTimestamp = 0\n",
    "    tg.maxTimestamp = audio_frames.size()[0] / 100\n",
    "\n",
    "    tier_name = 'phones'\n",
    "    phones_tier = tgio.IntervalTier(tier_name, [], minT=0, maxT=tg.maxTimestamp)\n",
    "\n",
    "    intervals = []\n",
    "    output_tokens = torch.argmax(outputs, dim=1)\n",
    "    decoded_output_tokens = vocab.decode(output_tokens)\n",
    "    prev, prevStart = None, 0\n",
    "    for i, token in enumerate(decoded_output_tokens):\n",
    "        if prev != token and prev:\n",
    "            intervals.append(Interval(prevStart, i/25, prev))\n",
    "            prev = token\n",
    "            prevStart = i/25\n",
    "        elif not prev:\n",
    "            prev = token\n",
    "    if prev:\n",
    "        intervals.append(Interval(prevStart, tg.maxTimestamp, prev))\n",
    "\n",
    "    new_phonemes_tier = phones_tier.new(entries=intervals)\n",
    "    tg.addTier(new_phonemes_tier)\n",
    "\n",
    "    tg.save(f'{output_dir}/{ids[0]}_{ids[1]}_{ids[2]}.TextGrid',\n",
    "            includeBlankSpaces=True,\n",
    "            format='long_textgrid',\n",
    "            reportingMode='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
